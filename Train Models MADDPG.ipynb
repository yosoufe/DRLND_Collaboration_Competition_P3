{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import Actor, MACritic\n",
    "from DDPG import MADDPG\n",
    "from tools import PlotTool, ReplayBuffer\n",
    "\n",
    "from unityagents import UnityEnvironment\n",
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from ipywidgets import *\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Enviornment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env         = UnityEnvironment(file_name=\"../Tennis_Linux/Tennis.x86_64\", no_graphics=True)\n",
    "brain_name  = env.brain_names[0]\n",
    "brain       = env.brains[brain_name]\n",
    "env_info    = env.reset(train_mode=True)[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_agents  = len(env_info.agents)\n",
    "action_size = brain.vector_action_space_size\n",
    "state_size  = len(env_info.vector_observations[0])\n",
    "print(\"state size:\",state_size, \"action size:\", action_size)\n",
    "print(\"state shape: \", env_info.vector_observations.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = []\n",
    "batch_size = 1024\n",
    "shared_buffer = ReplayBuffer(batch_size=batch_size, buffer_size=300 * 1000, seed=1, device=device)\n",
    "\n",
    "for _ in range(num_agents):\n",
    "    agent = MADDPG(state_size=state_size, \n",
    "                   action_size=action_size, \n",
    "                   actor_model=Actor,\n",
    "                   critic_model=MACritic,\n",
    "                   device=device,\n",
    "                   num_agents= 1, # number of non-interacting agents,\n",
    "                   num_interacting_agents = 2,\n",
    "                   seed=1,\n",
    "                   tau=1e-1,\n",
    "                   batch_size=batch_size,\n",
    "                   discount_factor = 0.99,\n",
    "                   actor_learning_rate=1e-4,\n",
    "                   critic_learning_rate=1e-3,\n",
    "                   replayBuffer= shared_buffer) #shared_buffer\n",
    "    agents.append(agent)\n",
    "agents[0].set_other_agent(agents[1])\n",
    "agents[1].set_other_agent(agents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def maddpg(n_episodes=1000, max_t=300, print_every=100):\n",
    "    scores_deque = deque(maxlen=print_every)\n",
    "    scores = []\n",
    "    score = np.zeros(num_agents)\n",
    "    \n",
    "    # commenting and plotting\n",
    "    desc = Label('Episode {}\\tAverage Score: {:.2f}'.format(0,0))\n",
    "    display(desc)\n",
    "    tqm = tqdm(range(1, n_episodes+1))\n",
    "    for i_episode in tqm:\n",
    "        env_info    = env.reset(train_mode=True)[brain_name]\n",
    "        states      = env_info.vector_observations\n",
    "        for agent in agents:\n",
    "            agent.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            statePlotter.push_date(states[0,:].reshape(1,state_size))\n",
    "            actions = None\n",
    "            for i, agent in enumerate(agents):\n",
    "                actions_with_noise, actions_without_noise  = agent.act(states[i],add_noise=True)\n",
    "                agent_actions = np.clip(actions_with_noise, -1, 1)\n",
    "                if actions is not None:\n",
    "                    actions = np.concatenate((actions,agent_actions),axis=0)\n",
    "                    actions_no_noise = np.concatenate((actions_no_noise,actions_without_noise),axis=0)\n",
    "                else:\n",
    "                    actions = agent_actions\n",
    "                    actions_no_noise = actions_without_noise\n",
    "                \n",
    "            env_info = env.step(actions)[brain_name]\n",
    "            next_states = env_info.vector_observations\n",
    "            reward = env_info.rewards\n",
    "            done = env_info.local_done\n",
    "            \n",
    "            # calculate next_actions from next_states for all agents\n",
    "            next_actions = None\n",
    "            for i, agent in enumerate(agents):\n",
    "                _, act_without_noise = agent.act(next_states[i],add_noise=False)\n",
    "                if next_actions is None:\n",
    "                    next_actions = act_without_noise\n",
    "                else:\n",
    "                    next_actions = np.vstack((next_actions, act_without_noise))\n",
    "            \n",
    "            for i, agent in enumerate(agents):\n",
    "                agent.step(this_state            = states[i],\n",
    "                           others_state          = np.delete(states, (i), axis=0).reshape(-1),\n",
    "                           this_action           = actions[i],\n",
    "                           others_action         = np.delete(actions, (i), axis=0).reshape(-1),\n",
    "                           reward                = reward[i],\n",
    "                           this_next_states      = next_states[i],\n",
    "                           others_next_states    = np.delete(next_states, (i), axis=0).reshape(-1), \n",
    "                           done                  = done[i],\n",
    "                          )\n",
    "            \n",
    "            states = next_states\n",
    "            score = score + np.array(reward)\n",
    "            \n",
    "            actionPlotterWithoutNoise.push_date(actions_no_noise.reshape(-1))\n",
    "            actionPlotter.push_date(actions.reshape(-1))\n",
    "            if np.any(done):\n",
    "                break \n",
    "        actionPlotter.draw(reset_for_next_time=True)\n",
    "        actionPlotterWithoutNoise.draw(reset_for_next_time=True)\n",
    "        statePlotter.draw(reset_for_next_time=True)\n",
    "        rewardPlot.push_date(score.reshape(-1))\n",
    "        rewardPlot.draw()\n",
    "        scores_deque.append(score)\n",
    "        scores.append(score)\n",
    "        score_mean_circular = np.mean(scores_deque)\n",
    "        desc.value = 'Episode {},'.format(i_episode)+ \\\n",
    "                     '\\tMinimum Score: {:.2f},'.format(np.min(scores_deque))+ \\\n",
    "                     '\\tAverage Score: {:.2f},'.format(score_mean_circular)\n",
    "        \n",
    "        \n",
    "        if score_mean_circular > 0.5:\n",
    "            # enough training\n",
    "            for i, agent in enumerate(agents):\n",
    "                agent.save_agent(f\"MADDPG_agent{i}\")\n",
    "            break\n",
    "            \n",
    "    return scores\n",
    "\n",
    "%matplotlib notebook\n",
    "rewardPlot = PlotTool(number_of_lines=num_agents, desc = \"R\" )\n",
    "actionPlotterWithoutNoise = PlotTool(number_of_lines=action_size*2, desc = \"AC\")\n",
    "actionPlotter = PlotTool(number_of_lines=action_size*2, desc = \"NAC\")\n",
    "statePlotter = PlotTool(number_of_lines=state_size, desc = \"S\")\n",
    "scores = maddpg(n_episodes=10000, max_t=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (RL Spin-up)",
   "language": "python",
   "name": "rl37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
